![Python](https://img.shields.io/badge/Python-3.10+-blue?style=flat-square&logo=python)
![Streamlit](https://img.shields.io/badge/Streamlit-1.53+-red?style=flat-square&logo=streamlit)
![Ollama](https://img.shields.io/badge/Ollama-Local_LLM-green?style=flat-square)
![Qdrant](https://img.shields.io/badge/Qdrant-Vector_DB-crimson?style=flat-square)

---

- **Frontend**: Streamlit (Custom CSS)
- **LLM**: Ollama + Qwen 2.5:14b
- **Embeddings**: BAAI/bge-m3 (1024 dims)
- **Reranker**: BAAI/bge-reranker-v2-m3
- **Vector DB**: Qdrant (Local persistent)
- **Memory**: Custom Mem0 implementation


### Y√™u c·∫ßu
- **Ollama** ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t v√† ch·∫°y

### C√°c b∆∞·ªõc c√†i ƒë·∫∑t
   ```powershell
   uv sync
   ```
   *Ho·∫∑c pip:* `pip install -r requirements.txt`

**T·∫£i Model cho Ollama**
   ```powershell
   ollama pull qwen2.5:14b
   ollama pull bge-m3
   ```

---

## üíª C√°ch Ch·∫°y ·ª®ng D·ª•ng

1. **Kh·ªüi ƒë·ªông Ollama Server**
   ```powershell
   ollama serve
   ```

2. **Ch·∫°y ·ª©ng d·ª•ng Streamlit**
   ```powershell
   uv run streamlit run app.py
   ```
   
3. **Truy c·∫≠p**: M·ªü tr√¨nh duy·ªát t·∫°i `http://localhost:8501`

```python
# C·∫•u h√¨nh LLM
LLM_MODEL_NAME = "qwen2.5:14b"    # ƒê·ªïi sang "qwen2.5:7b" n·∫øu m√°y y·∫øu
LLM_TEMP = 0.1                    # ƒê·ªô s√°ng t·∫°o (th·∫•p = ch√≠nh x√°c h∆°n)

# C·∫•u h√¨nh Chunking (Quan tr·ªçng cho hi·ªáu nƒÉng ingestion)
CHUNK_SIZE = 1000                 # K√≠ch th∆∞·ªõc m·ªói ƒëo·∫°n vƒÉn b·∫£n
CHUNK_OVERLAP = 200               # ƒê·ªô ch·ªìng l·∫∑p ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh



